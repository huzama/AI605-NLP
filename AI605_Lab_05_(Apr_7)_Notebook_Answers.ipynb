{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI605 Lab 05 (Apr 7) Notebook Answers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Quiz 05 Answers\n",
        "1. False\n",
        "2. False\n",
        "3. True\n",
        "4. False\n",
        "5. [0.2, 0.8]\n",
        "6. [0.0, 1.0]"
      ],
      "metadata": {
        "id": "ISfg9HrKOByh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective 1: Implement teacher forcing in encoder-decoder\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(605)\n",
        "batch_size = 4\n",
        "input_length = 16\n",
        "output_length = 8\n",
        "hidden_size = 32\n",
        "vocab_size = 64\n",
        "\n",
        "encoder = nn.LSTM(hidden_size, hidden_size, 1) # input size, hidden size, num layers\n",
        "decoder = nn.LSTM(2 * hidden_size, hidden_size, 1)\n",
        "embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "output_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "source_ids = (torch.rand(input_length, batch_size) * vocab_size).long()\n",
        "target_ids = (torch.rand(output_length, batch_size) * vocab_size).long()\n",
        "\n",
        "input_emb = embedding(source_ids)\n",
        "output_emb = torch.cat([torch.zeros(1, batch_size, hidden_size), embedding(target_ids[1:, :])], 0) # y_{t-1}, i.e. teacher forcing\n",
        "\n",
        "encoder_output, (hn, cn) = encoder(input_emb) # hn.size() = [1, batch_size, hidden_size]\n",
        "decoder_input = torch.cat([output_emb, hn.repeat(output_length, 1, 1)], 2)\n",
        "decoder_output, _ = decoder(decoder_input)\n",
        "logits = output_layer(decoder_output)\n",
        "\n",
        "cel = nn.CrossEntropyLoss()\n",
        "loss = cel(logits.reshape(output_length * batch_size, vocab_size), target_ids.reshape(output_length * batch_size))\n",
        "assert (loss - 4.1539).abs() < 0.01"
      ],
      "metadata": {
        "id": "1wgAuOk6OW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective 2: Implement a simple (Luong) attention mechanism in encoder-decoder\n",
        "attention_layer = nn.Linear(hidden_size, hidden_size)\n",
        "attention_logits = attention_layer(decoder_output.transpose(0, 1)) @ encoder_output.transpose(0, 1).transpose(1, 2) # tWs = (W @ t)^t @ s\n",
        "softmax = nn.Softmax(2)\n",
        "attention_weights = softmax(attention_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upm0Y_wsRY5p",
        "outputId": "6515b416-456c-41f2-e922-ce72d4d2085d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 4, 32])\n",
            "torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OyySbhuBgNk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}