{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI605 Lab 06 (Apr 14) Notebook Answers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Quiz 06 Answers\n",
        "1. True\n",
        "2. False\n",
        "3. False\n",
        "4. True\n",
        "5. True"
      ],
      "metadata": {
        "id": "CxU7Ito-xy4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 06 Objectives\n",
        "1. Implement Attention(Q, K, V) function of Transformer\n",
        "2. Implement MultiHead(Q, K, V) function of Transformer\n",
        "3. Create a mask for the autoregressive characteristic of Transformer decoder\n"
      ],
      "metadata": {
        "id": "qh_BAa3EFYik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49UD_vZZxwJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b82cf2-08ee-4842-cd88-813507eeddda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 16, 32])\n"
          ]
        }
      ],
      "source": [
        "# Objective 1: Implement Attention(Q, K, V) function of Transformer\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def attention(Q, K, V, mask=None):\n",
        "  softmax = nn.Softmax(2)\n",
        "  scale = 1.0 / torch.sqrt(torch.tensor(Q.size(2)))\n",
        "  logits = scale * Q @ K.transpose(1, 2)\n",
        "  if mask is not None:\n",
        "    logits += mask\n",
        "  out = softmax(logits) @ V\n",
        "  return out\n",
        "\n",
        "torch.manual_seed(605)\n",
        "\n",
        "batch_size = 8\n",
        "seq_len = 16\n",
        "hidden_size = 32\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, hidden_size)\n",
        "K = torch.randn(batch_size, seq_len, hidden_size)\n",
        "V = torch.randn(batch_size, seq_len, hidden_size)\n",
        "out = attention(Q, K, V)\n",
        "\n",
        "print(out.size())\n",
        "assert (out.max() - 1.204).abs() < 0.01"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective 2: Implement MultiHead(Q, K, V) function of Transformer\n",
        "from torch import nn\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "  def __init__(self, hidden_size, num_heads):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_heads = num_heads\n",
        "    self.head_size = int(hidden_size / num_heads)\n",
        "    self.q_layer = nn.Linear(hidden_size, hidden_size)\n",
        "    self.k_layer = nn.Linear(hidden_size, hidden_size)\n",
        "    self.v_layer = nn.Linear(hidden_size, hidden_size)\n",
        "  \n",
        "  def forward(self, Q, K, V):\n",
        "    batch_size, seq_len, hidden_size = Q.size()\n",
        "    \n",
        "    def preprocess(M):\n",
        "      Mi = M.reshape(batch_size, seq_len, self.num_heads, self.head_size)\n",
        "      M_ = Mi.transpose(1, 2).reshape(batch_size * self.num_heads, seq_len, self.head_size)\n",
        "      return M_\n",
        "\n",
        "    Q_ = preprocess(self.q_layer(Q))\n",
        "    K_ = preprocess(self.k_layer(K))\n",
        "    V_ = preprocess(self.v_layer(V))\n",
        "\n",
        "    out_ = attention(Q_, K_, V_)\n",
        "\n",
        "    # postprocess\n",
        "    outi = out_.reshape(batch_size, self.num_heads, seq_len, self.head_size).transpose(1, 2)\n",
        "    out = outi.reshape(batch_size, seq_len, self.num_heads * self.head_size)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "torch.manual_seed(605)\n",
        "\n",
        "batch_size = 8\n",
        "seq_len = 16\n",
        "hidden_size = 32\n",
        "num_heads = 4\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, hidden_size)\n",
        "K = torch.randn(batch_size, seq_len, hidden_size)\n",
        "V = torch.randn(batch_size, seq_len, hidden_size)\n",
        "multi_head = MultiHead(hidden_size, num_heads)\n",
        "out = multi_head(Q, K, V)\n",
        "\n",
        "print(out.size())\n",
        "assert (out.max() - 0.5557).abs() < 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cINTilG4Il9S",
        "outputId": "dd829e27-8847-4836-b87a-02c1e2a39afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 16, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective 3: Create a mask for the autoregressive characteristic of Transformer decoder\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def get_ar_mask(seq_len):\n",
        "  binary_mask = torch.ones(seq_len, seq_len).tril()\n",
        "  logit_mask = (1.0 - binary_mask) * -1e9\n",
        "  return logit_mask\n",
        "\n",
        "torch.manual_seed(605)\n",
        "\n",
        "batch_size = 8\n",
        "seq_len = 16\n",
        "hidden_size = 32\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, hidden_size)\n",
        "K = torch.randn(batch_size, seq_len, hidden_size)\n",
        "V = torch.randn(batch_size, seq_len, hidden_size)\n",
        "ar_mask = get_ar_mask(seq_len)\n",
        "out = attention(Q, K, V, mask=ar_mask)\n",
        "\n",
        "print(ar_mask.size())\n",
        "assert (out.max() - 3.8989).abs() < 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-d-a0L0ZGVO",
        "outputId": "6c595667-a8ee-4ce7-ac21-0c26522e1dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X3AjkrtAbYdw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}