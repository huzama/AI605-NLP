{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI605 Lab 08 (May 12) Notebook Answers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 08 Objectives\n",
        "1. Compute perplexity of a language model\n",
        "2. Create a character-level unigram language model\n",
        "3. Create a character-level n-gram language model with Laplace Smoothing\n",
        "\n"
      ],
      "metadata": {
        "id": "qh_BAa3EFYik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_text = \"The Korea Advanced Institute of Science and Technology (KAIST) is a national research university located in Daedeok Innopolis, Daejeon, South Korea. KAIST was established by the Korean government in 1971 as the nation's first public, research-oriented science and engineering institution.[3] KAIST is considered to be one of the most prestigious universities in the nation.[4][5] KAIST has been internationally accredited in business education,[6] and hosting the Secretariat of the Association of Asia-Pacific Business Schools (AAPBS).[7] KAIST has 10,504 full-time students and 1,342 faculty researchers (as of Fall 2019 Semester) and had a total budget of US$765 million in 2013, of which US$459 million was from research contracts.\"\n",
        "test_text = \"In 2007, KAIST partnered with international institutions and adopted dual degree programs for its students. Its partner institutions include the Technical University of Denmark,[8] Carnegie Mellon University,[9] the Georgia Institute of Technology,[10] the Technical University of Berlin,[11] and the Technical University of Munich.[12]\""
      ],
      "metadata": {
        "id": "hkO_jx5_RbWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Compute perplexity of a language model\n",
        "import numpy as np\n",
        "\n",
        "class CharLanguageModel(object):\n",
        "  def __init__(self):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def train(self, text):\n",
        "    raise NotImplementedError()\n",
        "  \n",
        "  # Get the probability of the next word given the prefix, in a dict format\n",
        "  def get_next_prob(self, prefix):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  # Get the perplexity of the text by this model\n",
        "  def get_perplexity(self, text):\n",
        "    log_prob = 0.0\n",
        "    for i, c in enumerate(text):\n",
        "      prefix = text[:i-1]\n",
        "      prob_dist = self.get_next_prob(prefix)\n",
        "      cur_log_prob = np.log(prob_dist[c])\n",
        "      log_prob += cur_log_prob\n",
        "    return np.exp(-log_prob / len(text))  "
      ],
      "metadata": {
        "id": "PIa6VQ9aibOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create a character-level unigram language model\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class UnigramCharLanguageModel(CharLanguageModel):\n",
        "  def __init__(self):\n",
        "    self.unigram_counts = None\n",
        "    self.all_count = None\n",
        "  \n",
        "  def train(self, text):\n",
        "    self.unigram_counts = Counter(text)\n",
        "    self.all_count = len(text)\n",
        "  \n",
        "  def get_next_prob(self, prefix):\n",
        "    # we don't use prefix at all\n",
        "    prob = {char: float(count)/self.all_count for char, count in self.unigram_counts.items()}\n",
        "    return prob"
      ],
      "metadata": {
        "id": "Q0AFC4jnhsrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a character-level n-gram language model with Laplace Smoothing\n",
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "vOg_e2fIiSmQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}